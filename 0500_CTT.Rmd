# (PART) Module 05 {-}

```{r include = FALSE}
#install.packages("devtools")
#devtools::install_github("talbano/epmr")
source("common.R")
library(tweetrmd) #... embedding tweets
library(tidyverse)
library(psych)

```

```{r links, child="links.md"}
```

# Welcome to Classical Test Theory

This module introduces the idea of classical test theory. Please watch the videos and work your way through the notes. **The videos start on the next page.**  You can find the video playlist for this module [here][pt4p-pl-05]. Most of the slides used to make the videos in this module can be found on canvas.



# Big Ideas in Classical Test Theory

```{r, echo=FALSE}

video_url="https://www.youtube.com/watch?v=zLeArwu2S2I"
embed_url(video_url) %>%
  use_align("center")
```

## Modeling Classic Test Theory

```{r, echo=FALSE}

video_url="https://www.youtube.com/watch?v=o_5DD74oa1E"
embed_url(video_url) %>%
  use_align("center")
```

## Model Applications

```{r, echo=FALSE}

video_url="https://www.youtube.com/watch?v=R2bQZUwSYGQ"
embed_url(video_url) %>%
  use_align("center")
```


# Systematic Error and Unreliability

```{r, echo=FALSE}

video_url="https://www.youtube.com/watch?v=agw9R1hcqK8"
embed_url(video_url) %>%
  use_align("center")
```


## Simulate a constant true score

In this example, I simulate a constant true score, and randomly varying error scores from a normal population with mean 0 and SD 1
Note, set.seed() gives R a starting point for generating random numbers, so we can get the same results on different computers

You should check the mean and SD of E and X. Creating a histogram of X might be interesting too...

```{r}
library(tidyverse)


set.seed(160416)
myt <- 20
mye <- rnorm(1000, mean = 0, sd = 1)
myx <- myt + mye

df=data.frame(myx,myt,mye)
mean(myx); sd(myx)

p <- ggplot(df, aes(x=myx)) + 
  geom_histogram()
p

```


## PISA total reading scores with simulated error and true scores based on CTT

```{r}

## Libraries

#install.packages("devtools")
#devtools::install_github("talbano/epmr")
library(epmr) 
library(ggplot2)


ritems <- c("r414q02", "r414q11", "r414q06", "r414q09", 
  "r452q03", "r452q04", "r452q06", "r452q07", "r458q01",
  "r458q07", "r458q04")

rsitems <- paste0(ritems, "s")

BEL_PISA09=PISA09[PISA09$cnt == "BEL", rsitems]

xscores <- rowSums(BEL_PISA09,
  na.rm = TRUE)
```

Simulate error scores based on known SEM of 1.4, which we'll calculate later, then create true scores
True scores are truncated to fall between 0 and 11 using setrange()

```{r}
escores <- rnorm(length(xscores), 0, 1.4)
tscores <- setrange(xscores - escores, y = xscores)

```

Combine in a data frame and create a scatterplot

```{r}

scores <- data.frame(x1 = xscores, t = tscores,
  e = escores)
ggplot(scores, aes(x1, t)) +
  geom_point(position = position_jitter(w = .3)) +
  geom_abline(col = "blue")
```


## Reliability and unreliability illustrated


Here we have simulated scores for a new form of the reading test called y. Note that rho is the made up reliability, which is set to 0.80, and x is the original reading total scores. Form y, which is slightly easier than x, has a mean of 6 and SD of 3.

```{r cttplot2, fig.cap = "PISA total reading scores and scores on a simulated second form of the reading test."}

xysim <- rsim(rho = .8, x = scores$x1, meany = 6, sdy = 3)
scores$y <- round(setrange(xysim$y, scores$x1))
ggplot(scores, aes(x1, y)) +
  geom_point(position = position_jitter(w = .3, h = .3)) +
  geom_abline(col = "blue")
```


# Domain Sampling Theory


```{r, echo=FALSE}

video_url="https://www.youtube.com/watch?v=XrkNO7hiKvE"
embed_url(video_url) %>%
  use_align("center")
```





# (PART) Module 06 {-}





# Installing epmr

A lot of the examples I'm using in the class require the `epmr` package. In theory, it should be really easy to install, using the following code.

```{r, eval=FALSE}
# Will install devtools if not already installed 
if (!require("devtools")) install.packages("devtools")	 
devtools::install_github("talbano/epmr")

```


It will install devtools, if you don't have it already and then compile the current version of the package for you. However, if your computer doesn't already have a compiler, you might have some trouble.


If you have trouble installing the `epmr` package, you are not alone. I, Prof. Mason, am working on resolving the issue. To install `epmr` package directly from github, you need `devtools` *and* a working development environment (a.k.a. an R friendly compiler). The specific compiler will depend on your operating system. 

  + Windows: Install [Rtools](http://cran.r-project.org/bin/windows/Rtools/).
  + Mac: Install Xcode from the Mac App Store.
  + Linux: Install a compiler and various development libraries (details vary across different flavors of Linux).

Try installing the compiler that corresponds to your operating system. And then try installing directly from github again.

```{r, eval=FALSE}
# Will install devtools if not already installed 
if (!require("devtools")) install.packages("devtools")	 
devtools::install_github("talbano/epmr")

```


If the compilation doesn't work, don't despair! I've compiled a version that *should* work. First download [this file "epmr_0.0.0.9000.tar.gz"](https://github.com/R-Computing-Lab/psychtesting/blob/main/data/epmr_0.0.0.9000.tar.gz) from github, and place it in your working directory. (Here is the direct [download link:]( https://github.com/R-Computing-Lab/psychtesting/raw/main/data/epmr_0.0.0.9000.tar.gz)) 

Then, install that package directly, using this code.

```{r, eval=FALSE}
install.packages("epmr_0.0.0.9000.tar.gz", repos = NULL, type = "source")
```


If that doesn't work, please try using the Rstudio GUI, as illustrated below.


1. Open the `tools` dropdown menu, and click `Install Packages`

```{r echo=FALSE, eval=TRUE}
knitr::include_graphics("img/install_step1.png")
```

2. Then change the install from setting to `Install from package archive`

```{r echo=FALSE, eval=TRUE}
knitr::include_graphics("img/install_step2.png")
```



3. Then select browse for file and locate the archive "epmr_0.0.0.9000.tar.gz"

```{r echo=FALSE, eval=TRUE}
knitr::include_graphics("img/install_step3.png")
```

4. Then select that archive, and you should see something like the image below


```{r echo=FALSE, eval=TRUE}
knitr::include_graphics("img/install_step4.png")
```


5. Press Install. This action should begin the installation process. *If you still cannot get the package to install, please let me know as soon as possible. That way, I can help you during regular business hours. (In other words, don't wait until the last minute)*





# Statistical definition of reliability. 

In CTT, reliability is defined as the proportion of variability in $X$ that is due to variability in true scores $T$:

\begin{equation}
r = \frac{\sigma^2_T}{\sigma^2_X}.
(\#eq:rel1)
\end{equation}


## Estimating reliability

One indirect estimate made possible by CTT is the correlation between scores on two forms of the same test:

\begin{equation}
r = \rho_{X_1 X_2} = \frac{\sigma_{X_1 X_2}}{\sigma_{X_1} \sigma_{X_2}}.
(\#eq:rel2)
\end{equation}

## Split-half method

The split-half method takes scores on a single test form, and separates them into scores on two halves of the test, which are treated as separate test forms. The correlation between these two halves then represents an indirect estimate of reliability, based on Equation \@ref(eq:rel1).

```{r}
# Split half correlation, assuming we only had scores on 
# one test form
# With an odd number of reading items, one half has 5 
# items and the other has 6
library(epmr)

xsplit1 <- rowSums(PISA09[PISA09$cnt == "BEL", 
  rsitems[1:5]])
xsplit2 <- rowSums(PISA09[PISA09$cnt == "BEL", 
  rsitems[6:11]])

cor(xsplit1, xsplit2, use = "complete")
```

## Spearman Brown

The Spearman-Brown formula was originally used to correct for the reduction in reliability that occurred when correlating two test forms that were only half the length of the original test. In theory, reliability will increase as we add items to a test. Thus, Spearman-Brown is used to estimate, or predict, what the reliability would be if the half-length tests were made into full-length tests.

```{r}
# sb_r() in the epmr package uses the Spearman-Brown 
# formula to estimate how reliability would change when 
# test length changes by a factor k
# If test length were doubled, k would be 2
sb_r(r = cor(xsplit1, xsplit2, use = "complete"), k = 2)
```

The Spearman-Brown reliability, $r_{new}$, is estimated as a function of what's labeled here as the old reliability, $r_{old}$, and the factor by which the length of $X$ is predicted to change, $k$:

\begin{equation}
r_{new} = \frac{kr_{old}}{(k - 1)r_{old} + 1}.
(\#eq:rnew)
\end{equation}


Again, $k$ is the factor by which the test length is increased or decreased. It is equal to the number of items in the new test divided by the number of items in the original test. Multiply $k$ by the old reliability, and then divided the result by $(k - 1)$ times the old reliability, plus 1. The epmr package contains `sb_r()`, a simple function for estimating the Spearman-Brown reliability.


## Standard Error of Measurement

Typically, we're more interested in how the unreliability of a test can be expressed in terms of the available observed variability. Thus, we multiply the unreliable proportion of variance by the standard deviation of $X$ to obtain the SEM:

\begin{equation}
SEM = \sigma_X\sqrt{1 - r}.
(\#eq:sem)
\end{equation}



Confidence intervals for `PISA09` can be estimated in the same way. First, we choose a measure of reliability, find the SD of observed scores, and obtain the corresponding SEM. Then, we can find the CI, which gives us the expected amount of uncertainty in our observed scores due to random measurement error. Here, we're calculating SEM and the CI using alpha, but other reliability estimates would work as well. Figure \@ref(fig:cttplot3) shows the 11 possible `PISA09` reading scores in order, with error bars based on SEM for students in Belgium.

```{r cttplot3, out.width='75%', fig.cap = "The PISA09 reading scale shown with 68 and 95 percent confidence intervals around each point."}
# Get alpha and SEM for students in Belgium

bela <- coef_alpha(PISA09[PISA09$cnt == "BEL", rsitems])
bela_alpha=bela$alpha
# The sem function from epmr also overlaps with sem from 
# another package so we're spelling it out here in long 
# form
belsem <- epmr::sem(r = bela_alpha, sd = sd(scores$x1,
  na.rm = TRUE))
# Plot the 11 possible total scores against themselves
# Error bars are shown for 1 SEM, giving a 68% confidence
# interval and 2 SEM, giving the 95% confidence interval
# x is converted to factor to show discrete values on the 
# x-axis
beldat <- data.frame(x = 1:11, sem = belsem)
ggplot(beldat, aes(factor(x), x)) +
  geom_errorbar(aes(ymin = x - sem * 2,
    ymax = x + sem * 2), col = "violet") +
  geom_errorbar(aes(ymin = x - sem, ymax = x + sem),
    col = "yellow") +
  geom_point()
```
Figure \@ref(fig:cttplot3) helps us visualize the impact of unreliable measurement on score comparisons. For example, note that the top of the 95% confidence interval for $X$ of 2 extends nearly to 5 points, and thus overlaps with the CI for adjacent scores 3 through 7. It isn't until $X$ of 8 that the CI no longer overlap. With a CI of `belsem` `r round(belsem, 3)`, we're 95% confident that students with observed scores differing at least by `belsem * 4` `r round(belsem, 3) * 4` have different true scores. Students with observed scores closer than `belsem * 4` may actually have the same true scores.



# Interpreting reliability and unreliability

There are no agreed-upon standards for interpreting reliability coefficients. Reliability is bound by 0 on the lower end and 1 at the upper end, because, by definition, the amount of true variability can never be less or more than the total available variability in $X$. Higher reliability is clearly better, but cutoffs for acceptable levels of reliability vary for different fields, situations, and types of tests. The stakes of a test are an important consideration when interpreting reliability coefficients. The higher the stakes, the higher we expect reliability to be. Otherwise, cutoffs depend on the particular application.

In general, reliabilities for educational and psychological tests can be interpreted using scales like the ones presented in Table \@ref(tab:interpretr). With medium-stakes tests, a reliability of 0.70 is sometimes considered minimally acceptable, 0.80 is decent, 0.90 is quite good, and anything above 0.90 is excellent. High stakes tests should have reliabilities at or above 0.90. Low stakes tests, which are often simpler and shorter than higher-stakes ones, often have reliabilities as low as 0.70. These are general guidelines, and interpretations can vary considerably by test. Remember that the cognitive measures in PISA would be considered low-stakes at the student level.

A few additional considerations are necessary when interpreting coefficient alpha. First, alpha assumes that all items measure the same single construct. Items are also assumed to be equally related to this construct, that is, they are assumed to be parallel measures of the construct. When the items are not parallel measures of the construct, alpha is considered a lower-bound estimate of reliability, that is, the true reliability for the test is expected to be higher than indicated by alpha. Finally, alpha is not a measure of dimensionality. It is frequently claimed that a strong coefficient alpha supports the unidimensionality of a measure. However, alpha does not index dimensionality. It is impacted by the extent to which all of the test items measure a single construct, but it does not necessarily go up or down as a test becomes more or less unidimensional.

```{r interpretr, echo=FALSE}
if (!knitr:::is_latex_output()) {
knitr::kable(cbind("Reliability" = c("$\\geq 0.90$", "$0.80 \\leq r < 0.90$", "$0.70 \\leq r < 0.80$", "$0.60 \\leq r < 0.70$", "$0.50 \\leq r < 0.60$", "$0.20 \\leq r < 0.50$", "$0.00 \\leq r < 0.20$"), "High Stakes Interpretation" = c("Excellent", "Good", "Acceptable", "Borderline", "Low", "Unacceptable", "Unacceptable"), "Low Stakes Interpretation" = c("Excellent", "Excellent", "Good", "Acceptable", "Borderline", "Low", "Unacceptable")),
  digits = 2, caption = "General Guidelines for Interpreting Reliability Coefficients")
} else {
knitr::kable(cbind("Reliability" = c("$>= 0.90$", "$0.80 <= r < 0.90$", "$0.70 <= r < 0.80$", "$0.60 <= r < 0.70$", "$0.50 <= r < 0.60$", "$0.20 <= r < 0.50$", "$0.00 <= r < 0.20$"), "High Stakes Interpretation" = c("Excellent", "Good", "Acceptable", "Borderline", "Low", "Unacceptable", "Unacceptable"), "Low Stakes Interpretation" = c("Excellent", "Excellent", "Good", "Acceptable", "Borderline", "Low", "Unacceptable")),
  digits = 2, caption = "General Guidelines for Interpreting Reliability Coefficients")
}

```


# Reliability study designs

Now that we've established the more common estimates of reliability and unreliability, we can discuss the four main study designs that allow us to collect data for our estimates. These designs are referred to as internal consistency, equivalence, stability, and equivalence/stability designs. Each design produces a corresponding type of reliability that is expected to be impacted by different sources of measurement error.

The four standard study designs vary in the number of test forms and the number of testing occasions involved in the study. Until now, we've been talking about using two test forms on two separate administrations. This study design is found in the lower right corner of Table \@ref(tab:rdesigns), and it provides us with an estimate of equivalence (for two different forms of a test) and stability (across two different administrations of the test). This study design has the potential to capture the most sources of measurement error, and it can thus produce the lowest estimate of reliability, because of the two factors involved. The more time that passes between administrations, and as two test forms differ more in their content and other features, the more error we would expect to be introduced. On the other hand, as our two test forms are administered closer in time, we move from the lower right corner to the upper right corner of Table \@ref(tab:rdesigns), and our estimate of reliability captures less of the measurement error introduced by the passage of time. We're left with an estimate of the equivalence between the two forms.

As our test forms become more and more equivalent, we eventually end up with the same test form, and we move to the first column in Table \@ref(tab:rdesigns), where one of two types of reliability is estimated. First, if we administer the same test twice with time passing between administrations, we have an estimate of the stability of our measurement over time. Given that the same test is given twice, any  measurement error will be due to the passage of time, rather than differences between the test forms. Second, if we administer one test only once, we no longer have an estimate of stability, and we also no longer have an estimate of reliability that is based on correlation. Instead, we have an estimate of what is referred to as the internal consistency of the measurement. This is based on the relationships among the test items themselves, which we treat as miniature alternate forms of the test. The resulting reliability estimate is impacted by error that comes from the items themselves being unstable estimates of the construct of interest.

```{r rdesigns, echo=FALSE}
rdesigns <- cbind("1 Form" = c("Internal Consistency", "Stability"),
  "2 Forms" = c("Equivalence", "Equivalence & Stability"))
rownames(rdesigns) <- c("1 Occasion", "2 Occasions")
knitr::kable(rdesigns, digits = 2, caption = "Four Main Reliability Study Designs")
```

Internal consistency reliability is estimated using either coefficient alpha or split-half reliability. All the remaining cells in Table \@ref(tab:rdesigns) involve estimates of reliability that are based on correlation coefficients.

Table \@ref(tab:rdesigns) contains four commonly used reliability study designs. There are others, including designs based on more than two forms or more than two occasions, and designs involving scores from raters, discussed below.

## Interrater reliability

> It was like listening to three cats getting strangled in an alley.  
> --- Simon Cowell, disparaging a singer on *American Idol*

Interrater reliability can be considered a specific instance of reliability where inconsistencies are not attributed to differences in test forms, test items, or administration occasions, but to the scoring process itself, where humans, or in some cases computers, contribute as raters. Measurement with raters often involves some form of performance assessment, for example, a stage performance within a singing competition. Judgment and scoring of such a performance by raters introduces additional error into the measurement process. Interrater reliability allows us to examine the negative impact of this error on our results.

Note that rater error is another factor or facet in the measurement process. Because it is another facet of measurement, raters can introduce error above and beyond error coming from sampling of items, differences in test forms, or the passage of time between administrations. This is made explicit within generalizability theory, discussed below. Some simpler methods for evaluating interrater reliability are introduced first.

### Proportion agreement

The proportion of agreement is the simplest measure of interrater reliability. It is calculated as the total number of times a set of ratings agree, divided by the total number of units of observation that are rated. The strengths of proportion agreement are that it is simple to calculate and it can be used with any type of *discrete* measurement scale. The major drawbacks are that it doesn't account for chance agreement between ratings, and it only utilizes the nominal information in a scale, that is, any ordering of values is ignored.

To see the effects of chance, let's simulate scores from two judges where ratings are completely random, as if scores of 0 and 1 are given according to the flip of a coin. Suppose 0 is tails and 1 is heads. In this case, we would expect two raters to agree a certain proportion of the time by chance alone. The `table()` function creates a cross-tabulation of frequencies, also known as a crosstab. Frequencies for agreement are found in the diagonal cells, from upper left to lower right, and frequencies for disagreement are found everywhere else.

```{r, append=TRUE, chap=5}
# Simulate random coin flips for two raters
# runif() generates random numbers from a uniform 
# distribution
flip1 <- round(runif(30))
flip2 <- round(runif(30))
table(flip1, flip2)
```
```{r, echo=FALSE, include=FALSE}
fliptab <- table(flip1, flip2)
```

Let's find the proportion agreement for the simulated coin flip data. The question we're answering is, how often did the coin flips have the same value, whether 0 or 1, for both raters across the 30 tosses? The crosstab shows this agreement in the first row and first column, with raters both flipping tails `r fliptab[1]` times, and in the second row and second column, with raters both flipping heads `r fliptab[4]` times. We can add these up to get `r sum(diag(fliptab))`, and divide by $n = 30$ to get the percentage agreement.

Data for the next few examples were simulated to represent scores given by two raters with a certain correlation, that is, a certain reliability. Thus, agreement here isn't simply by chance. In the population, scores from these raters correlated at 0.90. The score scale ranged from 0 to 6 points, with means set to 4 and 3 points for the raters 1 and 2, and SD of 1.5 for both. We'll refer to these as essay scores, much like the essay scores on the analytical writing section of the GRE. Scores were also dichotomized around a hypothetical cut score of 3, resulting in either a "Fail" or "Pass."

```{r, append=TRUE, chap=5}
# Simulate essay scores from two raters with a population 
# correlation of 0.90, and slightly different mean scores,
# with score range 0 to 6
# Note the capital T is an abbreviation for TRUE
essays <- rsim(100, rho = .9, meanx = 4, meany = 3,
  sdx = 1.5, sdy = 1.5, to.data.frame = T)
colnames(essays) <- c("r1", "r2")
essays <- round(setrange(essays, to = c(0, 6)))

# Use a cut off of greater than or equal to 3 to determine
# pass versus fail scores
# ifelse() takes a vector of TRUEs and FALSEs as its first
# argument, and returns here "Pass" for TRUE and "Fail" 
# for FALSE
essays$f1 <- factor(ifelse(essays$r1 >= 3, "Pass",   "Fail"))
essays$f2 <- factor(ifelse(essays$r2 >= 3, "Pass",   "Fail"))
table(essays$f1, essays$f2)
```

```{r, echo=FALSE}
etab <- table(essays$f1, essays$f2)
```

The upper left cell in the `table()` output above shows that for `r etab[1]` individuals, the two raters both gave "Fail." In the lower right cell, the two raters both gave "Pass" `r etab[4]` times. Together, these two totals represent the agreement in ratings, `r sum(diag(etab))` . The other cells in the table contain disagreements, where one rater said "Pass" but the other said "Fail." Disagreements happened a total of `r sum(etab) - sum(diag(etab))` times. Based on these totals, what is the proportion agreement in the pass/fail ratings?

Table \@ref(tab:pa1) shows the full crosstab of raw scores from each rater, with scores from rater 1 (`essays$r1`) in rows and rater 2 (`essays$r2`) in columns. The bunching of scores around the diagonal from upper left to lower right shows the tendency for agreement in scores.

```{r pa1, echo=FALSE}
knitr::kable(table(essays$r1, essays$r2), caption = "Crosstab of Scores From Rater 1 in Rows and Rater 2 in Columns")
```

Proportion agreement for the full rating scale, as shown in Table \@ref(tab:pa1), can be calculated by summing the agreement frequencies within the diagonal elements of the table, and dividing by the total number of people.

```{r, append=TRUE, chap=5}
# Pull the diagonal elements out of the crosstab with 
# diag(), sum them, and divide by the number of people
sum(diag(table(essays$r1, essays$r2))) / nrow(essays)
```

Finally, let's consider the impact of chance agreement between one of the hypothetical human raters and a monkey who randomly applies ratings, regardless of the performance that is demonstrated, as with a coin toss.

```{r, append=TRUE, chap=5}
# Randomly sample from the vector c("Pass", "Fail"), 
# nrow(essays) times, with replacement
# Without replacement, we'd only have 2 values to sample 
# from
monkey <- sample(c("Pass", "Fail"), nrow(essays),
  replace = TRUE)
table(essays$f1, monkey)
```

The results show that the hypothetical rater agrees with the monkey `r sum(diag(table(essays$f1, monkey)))/nrow(essays) * 100` percent of the time. Because we know that the monkey's ratings were completely random, we know that this proportion agreement is due entirely to chance.

### Kappa agreement

Proportion agreement is useful, but because it does not account for chance agreement, it should not be used as the only measure of interrater consistency. Kappa agreement is simply an adjusted form of proportion agreement that takes chance agreement into account.

Equation \@ref(eq:kappa1) contains the formula for calculating kappa for two raters.

\begin{equation}
\kappa = \frac{P_o - P_c}{1 - P_c}
(\#eq:kappa1)
\end{equation}

To obtain kappa, we first calculate the proportion of agreement, $P_o$, as we did with the proportion agreement. This is calculated as the total for agreement divided by the total number of people being rated. Next we calculate the chance agreement, $P_c$, which involves multiplying the row and column proportions (row and column totals divided by the total total) from the crosstab and then summing the result, as shown in Equation \@ref(eq:kappa2).

\begin{equation}
P_c = P_{first-row}P_{first-col} + P_{next-row}P_{next-col} + \dots + P_{last-row}P_{last-col}
(\#eq:kappa2)
\end{equation}

You do not need to commit Equations \@ref(eq:kappa1) and \@ref(eq:kappa2) to memory. Instead, they're included here to help you understand that kappa involves removing chance agreement from the observed agreement, and then dividing this observed non-chance agreement by the total possible non-chance agreement, that is, $1 - P_c$.

The denominator for the kappa equation contains the maximum possible agreement beyond chance, and the numerator contains the actual observed agreement beyond chance. So, the maximum possible kappa is 1.0. In theory, we shouldn't ever observe less agreement than than expected by chance, which means that kappa should never be negative. Technically it is possible to have kappa below 0. When kappa is below 0, it indicates that our observed agreement is below what we'd expect due to chance. Kappa should also be no larger than proportion agreement. In the example data, the proportion agreement decreased from `r format(round(astudy(essays[, 1:2])[1], 3))` to `r format(round(astudy(essays[, 1:2])[2], 3))` for kappa.

A weighted version of the kappa index is also available. Weighted kappa let us reduce the negative impact of partial disagreements relative to more extreme disagreements in scores, by taking into account the ordinal nature of a score scale. For example, in Table \@ref(tab:pa1), notice that only the diagonal elements of the crosstab measure perfect agreement in scores, and all other elements measure disagreements, even the ones that are close together like 2 and 3. With weighted kappa, we can give less weight to these smaller disagreements and more weight to larger disagreements such as scores of 0 and 6 in the lower left and upper right of the table. This weighting ends up giving us a higher agreement estimate.

Here, we use the function `astudy()` from epmr to calculate proportion agreement, kappa, and weighted kappa indices. Weighted kappa gives us the highest estimate of agreement. Refer to the documentation for `astudy()` to see how the weights are calculated. 

```{r, append=TRUE, chap=5}
# Use the astudy() function from epmr to measure agreement
astudy(essays[, 1:2])
```

### Pearson correlation

The Pearson correlation coefficient, introduced above for CTT reliability, improves upon agreement indices by accounting for the ordinal nature of ratings without the need for explicit weighting. The correlation tells us how consistent raters are in their rank orderings of individuals. Rank orderings that are closer to being in agreement are automatically given more weight when determining the overall consistency of scores.

The main limitation of the correlation coefficient is that it ignores *systematic differences* in ratings when focusing on their rank orders. This limitation has to do with the fact that correlations are oblivious to linear transformations of score scales. We can modify the mean or standard deviation of one or both variables being correlated and get the same result. So, if two raters provide consistently different ratings, for example, if one rater is more forgiving overall, the correlation coefficient can still be high as long as the rank ordering of individuals does not change.

This limitation is evident in our simulated essay scores, where rater 2 gave lower scores on average than rater 1. If we subtract 1 point from every score for rater 2, the scores across raters will be more similar, as shown in Figure \@ref(fig:essays), but we still get the same interrater reliability.

```{r, append=TRUE, chap=5}
# Certain changes in descriptive statistics, like adding 
# constants won't impact correlations
cor(essays$r1, essays$r2)
dstudy(essays[, 1:2])
cor(essays$r1, essays$r2 + 1)
```

A systematic difference in scores can be visualized by a consistent vertical or horizontal shift in the points within a scatter plot. Figure \@ref(fig:essays) shows that as scores are shifted higher for rater 2, they are more consistent with rater 1 in an absolute sense, despite the fact that the underlying linear relationship remains unchanged.

```{r essays, append=TRUE, chap=5, fig.show='hold', out.width='50%', fig.cap="Scatter plots of simulated essay scores with a systematic difference around 0.5 points."}
# Comparing scatter plots
ggplot(essays, aes(r1, r2)) +
  geom_point(position = position_jitter(w = .1, h = .1)) +
  geom_abline(col = "blue")
ggplot(essays, aes(r1, r2 + 1)) +
  geom_point(position = position_jitter(w = .1, h = .1)) +
  geom_abline(col = "blue")
```

Is it a problem that the correlation ignores systematic score differences? Can you think of any real-life situations where it wouldn't be cause for concern? A simple example is when awarding scholarships or giving other types of awards or recognition. In these cases consistent rank ordering is key and systematic differences are less important because the purpose of the ranking is to identify the top candidate. There is no absolute scale on which subjects are rated. Instead, they are rated in comparison to one another. As a result, a systematic difference in ratings would technically not matter.


## Summary

This module provided an overview of reliability within the frameworks of CTT, for items and test forms, for reliability study designs with multiple facets. After a general definition of reliability in terms of consistency in scores, the CTT model was introduced, and two commonly used indices of CTT reliability were discussed: correlation and coefficient alpha. Reliability was then presented as it relates to consistency of scores from raters. Inter-rater agreement indices were compared, along with the correlation coefficient.




### Exercises

1. Use the R object `scores` to find the average variability in `x1` for a given value on `t`. How does this compare to the SEM?
2. Use `PISA09` to calculate split-half reliabilities with different combinations of reading items in each half. Then, use these in the Spearman-Brown formula to estimate reliabilities for the full-length test. Why do the results differ?
3. Suppose you want to reduce the SEM for a final exam in a course you are teaching. Identify three sources of measurement error that could contribute to the SEM, and three that could not. Then, consider strategies for reducing error from these sources.
4. Estimate the internal consistency reliability for the attitude toward school scale. Remember to reverse code items as needed.
5. Dr. Phil is developing a measure of relationship quality to be used in counseling settings with couples. He intends to administer the measure to couples multiple times over a series of counseling sessions. Describe an appropriate study design for examining the reliability of this measure.
6. More and more TV shows lately seem to involve people performing some talent on stage and then being critiqued by a panel of judges, one of whom is British. Describe the ``true score'' for a performer in this scenario, and identify sources of measurement error that could result from the judging process, including both systematic and random sources of error.
7. With proportion agreement, consider the following questions. When would we expect to see 0% or nearly 0% agreement, if ever? What would the counts in the table look like if there were 0% agreement? When would we expect to see 100% or nearly 100% agreement, if ever?  What would the counts in the table look like if there were 100% agreement?
8. What is the maximum possible value for kappa? And what would we expect the minimum possible value to be?
9. Given the strengths and limitations of correlation as a measure of interrater reliability, with what type of score referencing is this measure of reliability most appropriate?
10. Compare the interrater agreement indices with interrater reliability based on Pearson correlation. What makes the correlation coefficient useful with interval data? What does it tell us, or what does it do, that an agreement index does not?

